# LLM Mock API Environment Variables
# Copy this file to .env and customize for your environment

# ASP.NET Core Settings
ASPNETCORE_ENVIRONMENT=Development
ASPNETCORE_URLS=http://+:8080

# LLM Configuration
MockLlmApi__BaseUrl=http://ollama:11434/v1/
MockLlmApi__ModelName=llama3
MockLlmApi__Temperature=1.2
MockLlmApi__TimeoutSeconds=30
MockLlmApi__MaxContextWindow=8192
MockLlmApi__EnableVerboseLogging=false

# Caching Configuration
MockLlmApi__EnableAutoChunking=true
MockLlmApi__MaxItems=1000
MockLlmApi__CacheSlidingExpirationMinutes=15
MockLlmApi__EnableCacheStatistics=true

# Context Configuration
MockLlmApi__ContextExpirationMinutes=15

# Streaming Configuration
MockLlmApi__SseMode=LlmTokens
MockLlmApi__EnableContinuousStreaming=false
MockLlmApi__ContinuousStreamingIntervalMs=2000

# Port Mappings (Host:Container)
LLMAPI_HTTP_PORT=5116
LLMAPI_HTTPS_PORT=5117
OLLAMA_PORT=11434
