mostlylucid.mockllmapi — Release Notes

===============================================================================
Release 2.1.0 — 2025-01-06
===============================================================================

NOTE: v2.0.0 was skipped to refine key areas before stable release:
      - Chunking reliability (array formatting)
      - Configuration clarity (streamlined appsettings + model reference)
      - Comprehensive validation suite (70+ HTTP tests)
      - Complete Swagger documentation for all management endpoints

Quality & Validation Release - Production Ready

1. HTTP Validation Suite (70+ Tests)
   - Complete coverage in LLMApi.http: OpenAPI, contexts, gRPC, SSE, chunking
   - Backend selection (X-LLM-Backend header)
   - Schema validation (includeSchema + X-Response-Schema)

2. Enhanced Chunking Reliability
   - Explicit array formatting: prevents {...},{...} → ensures [{...},{...}]
   - Updated PromptBuilder.cs and ChunkingCoordinator.cs
   - Model/temperature recommendations documented

3. Streamlined Configuration
   - Clean appsettings.json (removed verbose comments)
   - NEW docs/OLLAMA_MODELS.md (10+ models, temp guidelines, troubleshooting)

4. Complete Documentation
   - Full Swagger docs for 25+ management endpoints
   - NEW docs/BACKEND_API_REFERENCE.md (600+ lines)

Chunking Note: At temperature 1.2+, LLMs may generate comma-separated objects.
Workarounds: Lower temp to 0.8-1.0, use llama3.2:3b/mistral-nemo, reduce
count, or set ?autoChunk=false

Fully backward compatible - no breaking changes

===============================================================================
Previous Releases
===============================================================================

v2.0.0 (RC only - production release skipped):
- SSE streaming modes (LlmTokens/CompleteObjects/ArrayItems)
- Multi-backend load balancing
- Swagger/OpenAPI UI
- Per-request backend selection (?backend= or X-LLM-Backend header)

v1.8.0: Auto-chunking for large responses, multiple LLM backends
v1.7.0: gRPC service mocking with .proto upload, JSON+binary Protobuf
v1.6.0: Error simulation, API context management
v1.5.0: API contexts, GraphQL support, Polly resilience
v1.0-1.2: Initial releases, SignalR streaming, modular architecture

Full history: github.com/ScottGalloway/mostlylucid.mockllmapi/blob/main/RELEASE_NOTES.md
