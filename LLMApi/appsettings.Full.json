{
  "Logging": {
    "LogLevel": {
      "Default": "Information",
      "Microsoft.AspNetCore": "Warning"
    }
  },
  "AllowedHosts": "*",O

  "MockLlmApi": {

    // ========================================
    // LEGACY SINGLE BACKEND CONFIGURATION
    // ========================================
    // These settings are maintained for backward compatibility.
    // If you configure "Backends" below, these are ignored.
    // Otherwise, they automatically create a default Ollama backend.

    "BaseUrl": "http://localhost:11434/v1/",
    "ModelName": "llama3",


    // ========================================
    // MULTIPLE LLM BACKENDS (v1.8.0+)
    // ========================================
    // Configure multiple LLM providers for load balancing, failover, and flexibility.
    // Per-request selection via: X-LLM-Backend header or ?backend= query param
    // See: docs/MULTIPLE_LLM_BACKENDS.md

    "Backends": [

      // --- OLLAMA BACKENDS (Local LLM Server) ---
      {
        "Name": "ollama-llama3",
        "Provider": "ollama",
        "BaseUrl": "http://localhost:11434/v1/",
        "ModelName": "llama3",
        "ApiKey": null,                    // Optional: Add API key if your Ollama requires auth
        "MaxTokens": 8192,                 // Model-specific token limit
        "Enabled": true,
        "Weight": 3,                       // Higher weight = more requests in load balancing
        "MaxConcurrentRequests": 0,        // 0 = unlimited
        "HealthCheckPath": null,           // Future: "/health" for monitoring
        "Priority": 10                     // Higher = preferred backend
      },
      {
        "Name": "ollama-mistral",
        "Provider": "ollama",
        "BaseUrl": "http://localhost:11434/v1/",
        "ModelName": "mistral",
        "ApiKey": null,
        "MaxTokens": 8192,
        "Enabled": true,
        "Weight": 2,
        "MaxConcurrentRequests": 0,
        "HealthCheckPath": null,
        "Priority": 5
      },
      {
        "Name": "ollama-codellama",
        "Provider": "ollama",
        "BaseUrl": "http://localhost:11434/v1/",
        "ModelName": "codellama",
        "ApiKey": null,
        "MaxTokens": 16384,
        "Enabled": false,                  // Disabled by default
        "Weight": 1,
        "MaxConcurrentRequests": 0,
        "HealthCheckPath": null,
        "Priority": 3
      },
      {
        "Name": "ollama-mistral-nemo",
        "Provider": "ollama",
        "BaseUrl": "http://localhost:11434/v1/",
        "ModelName": "mistral-nemo",
        "ApiKey": null,
        "MaxTokens": 128000,               // 128k context window!
        "Enabled": false,                  // Enable this to test huge contexts
        "Weight": 2,
        "MaxConcurrentRequests": 0,
        "HealthCheckPath": null,
        "Priority": 6,
        "Description": "Mistral-Nemo 12B with 128k context - ideal for massive result sets and complex nested data"
      },

      // --- OPENAI BACKENDS (Cloud API) ---
      {
        "Name": "openai-gpt4-turbo",
        "Provider": "openai",
        "BaseUrl": "https://api.openai.com/v1/",
        "ModelName": "gpt-4-turbo",
        "ApiKey": "sk-proj-YOUR-API-KEY-HERE",  // REQUIRED for OpenAI
        "MaxTokens": 32768,
        "Enabled": false,                  // Disabled to avoid unexpected charges
        "Weight": 1,
        "MaxConcurrentRequests": 5,        // Rate limiting
        "HealthCheckPath": null,
        "Priority": 8
      },
      {
        "Name": "openai-gpt4",
        "Provider": "openai",
        "BaseUrl": "https://api.openai.com/v1/",
        "ModelName": "gpt-4",
        "ApiKey": "sk-proj-YOUR-API-KEY-HERE",
        "MaxTokens": 8192,
        "Enabled": false,
        "Weight": 1,
        "MaxConcurrentRequests": 5,
        "HealthCheckPath": null,
        "Priority": 7
      },
      {
        "Name": "openai-gpt35-turbo",
        "Provider": "openai",
        "BaseUrl": "https://api.openai.com/v1/",
        "ModelName": "gpt-3.5-turbo",
        "ApiKey": "sk-proj-YOUR-API-KEY-HERE",
        "MaxTokens": 4096,
        "Enabled": false,
        "Weight": 2,
        "MaxConcurrentRequests": 10,
        "HealthCheckPath": null,
        "Priority": 5
      },

      // --- LM STUDIO BACKENDS (Local Server) ---
      {
        "Name": "lmstudio-default",
        "Provider": "lmstudio",
        "BaseUrl": "http://localhost:1234/v1/",
        "ModelName": "local-model",        // Model name from LM Studio
        "ApiKey": null,                    // Usually not required
        "MaxTokens": 8192,
        "Enabled": false,
        "Weight": 1,
        "MaxConcurrentRequests": 0,
        "HealthCheckPath": null,
        "Priority": 4
      },
      {
        "Name": "lmstudio-experimental",
        "Provider": "lmstudio",
        "BaseUrl": "http://localhost:1234/v1/",
        "ModelName": "experimental-model",
        "ApiKey": null,
        "MaxTokens": 4096,
        "Enabled": false,
        "Weight": 1,
        "MaxConcurrentRequests": 0,
        "HealthCheckPath": null,
        "Priority": 1
      },

      // --- REMOTE OLLAMA INSTANCES ---
      {
        "Name": "ollama-remote-server",
        "Provider": "ollama",
        "BaseUrl": "http://192.168.1.100:11434/v1/",  // Remote server IP
        "ModelName": "llama3",
        "ApiKey": null,
        "MaxTokens": 8192,
        "Enabled": false,
        "Weight": 1,
        "MaxConcurrentRequests": 0,
        "HealthCheckPath": null,
        "Priority": 2
      }
    ],


    // ========================================
    // LLM GENERATION SETTINGS
    // ========================================

    "Temperature": 1.2,                   // 0.0 = deterministic, 2.0 = very creative/random
    "MaxInputTokens": 4096,               // Maximum input context size
    "MaxOutputTokens": 2048,              // Maximum output tokens (used for chunking)
    "TimeoutSeconds": 30,                 // HTTP timeout for LLM requests


    // ========================================
    // SSE STREAMING MODES (v1.8.0+)
    // ========================================
    // Configure Server-Sent Events streaming behavior.
    // See: docs/SSE_STREAMING_MODES.md

    "SseMode": "LlmTokens",               // LlmTokens | CompleteObjects | ArrayItems
    //   - LlmTokens: Token-by-token streaming (AI chat interfaces)
    //   - CompleteObjects: Complete objects per event (Twitter API, stock tickers)
    //   - ArrayItems: Array items with metadata (paginated results, bulk exports)
    // Override per-request: ?sseMode=CompleteObjects


    // ========================================
    // AUTOMATIC REQUEST CHUNKING (v1.8.0+)
    // ========================================
    // Automatically splits large requests into chunks to fit within token limits.
    // See: CHUNKING_AND_CACHING.md

    "EnableAutoChunking": true,           // Enable automatic chunking
    "MaxItems": 1000,                     // Max items per response AND cache size


    // ========================================
    // CACHING CONFIGURATION (v1.8.0+)
    // ========================================
    // Cache LLM responses to reduce latency and API costs.
    // See: CHUNKING_AND_CACHING.md

    "MaxCachePerKey": 5,                  // Cached variants per unique request
    "CacheSlidingExpirationMinutes": 15,  // Expire after inactivity
    "CacheAbsoluteExpirationMinutes": 60, // Maximum cache lifetime
    "CacheRefreshThresholdPercent": 50,   // Future: pre-fetch threshold
    "CachePriority": 1,                   // 0=Low, 1=Normal, 2=High, 3=NeverRemove
    "EnableCacheStatistics": false,       // Track cache hits/misses
    "EnableCacheCompression": false,      // Compress cached responses


    // ========================================
    // RESILIENCE POLICIES (Polly v8)
    // ========================================
    // Automatic retry and circuit breaker patterns for reliability.

    "EnableRetryPolicy": true,
    "MaxRetryAttempts": 3,                // Retry up to 3 times
    "RetryBaseDelaySeconds": 1.0,         // Exponential backoff: 1s, 2s, 4s

    "EnableCircuitBreaker": true,
    "CircuitBreakerFailureThreshold": 5,  // Open circuit after 5 failures
    "CircuitBreakerDurationSeconds": 30,  // Stay open for 30 seconds


    // ========================================
    // CUSTOM PROMPT TEMPLATES
    // ========================================
    // Override default prompts with custom templates.
    // Placeholders: {method}, {path}, {body}, {randomSeed}, {timestamp}

    "CustomPromptTemplate": null,         // null = use default
    "CustomStreamingPromptTemplate": null,


    // ========================================
    // DELAY SIMULATION
    // ========================================
    // Add realistic delays to simulate network latency.

    // Random delay before responding to any request
    "RandomRequestDelayMinMs": 0,
    "RandomRequestDelayMaxMs": 0,         // Random between Min and Max

    // Delay between streaming chunks (SSE)
    "StreamingChunkDelayMinMs": 0,
    "StreamingChunkDelayMaxMs": 0,


    // ========================================
    // RESPONSE OPTIONS
    // ========================================

    "IncludeShapeInResponse": false,      // Include JSON schema in response
    "EnableVerboseLogging": false,        // Detailed logging output


    // ========================================
    // GRAPHQL CONFIGURATION
    // ========================================
    // Settings for /graphql endpoint.

    "GraphQLMaxTokens": 500,              // 200-300 for small models, 500-1000 for large


    // ========================================
    // SIGNALR REAL-TIME DATA
    // ========================================
    // Configure SignalR hubs for real-time streaming data.

    "SignalRPushIntervalMs": 5000,        // Push interval (5 seconds)
    "HubContexts": [
      {
        "Name": "weather",
        "Description": "Realistic weather data with temperature, conditions, humidity, and wind speed",
        "Method": "GET",
        "Path": "/data/weather",
        "Body": null,
        "Shape": "{\"temperature\":25,\"condition\":\"sunny\",\"humidity\":60,\"windSpeed\":10}",
        "IsJsonSchema": false,
        "ApiContextName": "weather-station-1",
        "BackendName": null,               // null = use default backend selection
        "BackendNames": null,
        "IsActive": false,
        "ConnectionCount": 0,
        "ErrorConfig": null                // null = normal data generation
      },
      {
        "Name": "stock-prices",
        "Description": "Real-time stock prices with ticker, price, change, and volume",
        "Method": "GET",
        "Path": "/data/stocks",
        "Body": null,
        "Shape": "{\"ticker\":\"AAPL\",\"price\":150.25,\"change\":2.5,\"volume\":1000000}",
        "IsJsonSchema": false,
        "ApiContextName": "market-data",
        "BackendName": "ollama-llama3",    // Fast local model for high-frequency simple data
        "BackendNames": null,
        "IsActive": false,
        "ConnectionCount": 0,
        "ErrorConfig": null
      },
      {
        "Name": "vehicle-tracking",
        "Description": "GPS tracking data for vehicles with location, speed, and status",
        "Method": "GET",
        "Path": "/data/vehicles",
        "Body": null,
        "Shape": "{\"vehicleId\":\"V001\",\"lat\":40.7128,\"lng\":-74.0060,\"speed\":45,\"status\":\"moving\"}",
        "IsJsonSchema": false,
        "ApiContextName": "fleet-tracker",
        "BackendName": "ollama-llama3",    // Rapid generation for live tracking data
        "BackendNames": null,
        "IsActive": false,
        "ConnectionCount": 0,
        "ErrorConfig": null
      },
      {
        "Name": "complex-analytics",
        "Description": "Complex business analytics dashboard with nested metrics, trends, and predictions",
        "Method": "POST",
        "Path": "/data/analytics",
        "Body": "{\"timeRange\":\"last30days\",\"metrics\":[\"revenue\",\"users\",\"churn\"]}",
        "Shape": null,                     // Let LLM determine structure from description
        "IsJsonSchema": false,
        "ApiContextName": "analytics-session",
        "BackendName": "openai-gpt4-turbo", // Use powerful model for complex nested data
        "BackendNames": null,
        "IsActive": false,
        "ConnectionCount": 0,
        "ErrorConfig": null
      },
      {
        "Name": "graphql-query-results",
        "Description": "GraphQL query results with deeply nested user profiles, posts, comments, and relationships",
        "Method": "POST",
        "Path": "/graphql",
        "Body": "{\"query\":\"{ users { id name posts { id title comments { id text author { name } } } } }\"}",
        "Shape": null,
        "IsJsonSchema": false,
        "ApiContextName": null,
        "BackendName": "openai-gpt4-turbo", // Complex relational data needs GPT-4
        "BackendNames": null,
        "IsActive": false,
        "ConnectionCount": 0,
        "ErrorConfig": null
      },
      {
        "Name": "iot-sensors-bulk",
        "Description": "High-volume IoT sensor readings from 1000+ devices",
        "Method": "GET",
        "Path": "/data/sensors",
        "Body": null,
        "Shape": "{\"deviceId\":\"sensor-001\",\"value\":23.5,\"unit\":\"celsius\",\"timestamp\":1234567890}",
        "IsJsonSchema": false,
        "ApiContextName": "iot-network",
        "BackendName": "ollama-llama3",    // Fast local model for simple repetitive data
        "BackendNames": null,              // null = use single BackendName
        "IsActive": false,
        "ConnectionCount": 0,
        "ErrorConfig": null
      },
      {
        "Name": "load-balanced-pool",
        "Description": "Example of load balancing across multiple backends for high throughput",
        "Method": "GET",
        "Path": "/data/load-balanced",
        "Body": null,
        "Shape": "{\"id\":1,\"value\":\"sample data\",\"timestamp\":1234567890}",
        "IsJsonSchema": false,
        "ApiContextName": "high-throughput",
        "BackendName": null,               // null when using BackendNames
        "BackendNames": [                  // Load balance across multiple backends!
          "ollama-llama3",                 // Weight: 3 (gets more requests)
          "ollama-mistral",                // Weight: 2 (gets fewer requests)
          "lmstudio-default"               // Weight: 1 (gets fewest requests)
        ],
        "IsActive": false,
        "ConnectionCount": 0,
        "ErrorConfig": null
      },
      {
        "Name": "massive-dataset-128k",
        "Description": "Massive dataset generation leveraging 128k context window - generates thousands of detailed records with complex nested structures including users, transactions, metadata, and relationships",
        "Method": "GET",
        "Path": "/data/massive",
        "Body": null,
        "Shape": "{\"users\":[{\"id\":1,\"name\":\"John Doe\",\"email\":\"john@example.com\",\"transactions\":[{\"id\":\"tx-001\",\"amount\":99.99,\"timestamp\":1234567890,\"metadata\":{\"category\":\"electronics\",\"tags\":[\"laptop\",\"warranty\"]}}]}]}",
        "IsJsonSchema": false,
        "ApiContextName": "massive-data-context",
        "BackendName": "ollama-mistral-nemo",  // 128k context for huge result sets!
        "BackendNames": null,
        "IsActive": false,
        "ConnectionCount": 0,
        "ErrorConfig": null,
        "Notes": "Test with MaxItems=10000+ to see Mistral-Nemo handle massive context. Use EnableAutoChunking=true to split into manageable chunks."
      },
      {
        "Name": "error-simulation",
        "Description": "Hub that always returns errors (for testing error handling)",
        "Method": "GET",
        "Path": "/data/errors",
        "Body": null,
        "Shape": null,
        "IsJsonSchema": false,
        "ApiContextName": null,
        "BackendName": null,               // Not applicable for error simulation
        "BackendNames": null,
        "IsActive": false,
        "ConnectionCount": 0,
        "ErrorConfig": {
          "Code": 500,
          "Message": "Internal Server Error",
          "Details": "Database connection failed"
        }
      }
    ],


    // ========================================
    // OPENAPI / SWAGGER SPECIFICATIONS
    // ========================================
    // Load OpenAPI specs to auto-generate mock endpoints.

    "OpenApiSpecs": [
      {
        "Name": "petstore",
        "Source": "https://petstore3.swagger.io/api/v3/openapi.json",
        "BasePath": "/petstore",          // Mount point
        "ContextName": "petstore-demo",   // Shared context for consistency
        "EnableStreaming": false,
        "IncludeTags": null,              // null = all tags
        "ExcludeTags": ["admin"],         // Exclude admin endpoints
        "IncludePaths": null,             // null = all paths
        "ExcludePaths": ["/internal/*"]   // Exclude internal APIs
      },
      {
        "Name": "local-api",
        "Source": "./specs/my-api.yaml",  // Local file path
        "BasePath": "/api/v1",
        "ContextName": null,              // No shared context
        "EnableStreaming": true,          // Add /stream endpoints
        "IncludeTags": ["users", "products"],
        "ExcludeTags": null,
        "IncludePaths": ["/users/*", "/products/*"],
        "ExcludePaths": null
      },
      {
        "Name": "external-api",
        "Source": "https://api.example.com/swagger.json",
        "BasePath": null,                 // Use default from spec
        "ContextName": "external-session",
        "EnableStreaming": false,
        "IncludeTags": null,
        "ExcludeTags": null,
        "IncludePaths": null,
        "ExcludePaths": null
      }
    ]
  }
}
