{
  "Logging": {
    "LogLevel": {
      "Default": "Information",
      "Microsoft.AspNetCore": "Warning"
    }
  },
  "AllowedHosts": "*",
  "MockLlmApi": {
    // ===================================================================
    // PROVIDER CONFIGURATION - Choose one provider and uncomment its section
    // ===================================================================

    // OPTION 1: Ollama (Local LLM - DEFAULT)
    "Provider": "Ollama",  // Options: Ollama, OpenAI, LMStudio, AzureOpenAI
    "BaseUrl": "http://localhost:11434",
    "ModelName": "llama3",  // or "mistral:7b", "phi3", "qwen3:14b", etc.
    "Temperature": 1.2,
    // Note: Ollama doesn't require an API key

    // OPTION 2: OpenAI (Cloud)
    // "Provider": "OpenAI",
    // "ModelName": "gpt-4o",  // or "gpt-4o-mini", "gpt-3.5-turbo"
    // "ApiKey": "your-api-key-here",  // or set OPENAI_API_KEY environment variable
    // "Temperature": 1.2,

    // OPTION 3: LMStudio (Local OpenAI-compatible server)
    // "Provider": "LMStudio",
    // "BaseUrl": "http://localhost:1234/v1",  // default LMStudio endpoint
    // "ModelName": "model-name-from-lmstudio",
    // "Temperature": 1.2,
    // Note: LMStudio doesn't require an API key

    // OPTION 4: Azure OpenAI (Microsoft Azure)
    // "Provider": "AzureOpenAI",
    // "BaseUrl": "https://your-resource.openai.azure.com",
    // "ModelName": "your-deployment-name",  // deployment name, not model name
    // "ApiKey": "your-azure-api-key",  // or set AZURE_OPENAI_API_KEY environment variable
    // "Temperature": 1.2,

    // ===================================================================
    // COMMON SETTINGS (apply to all providers)
    // ===================================================================
    "TimeoutSeconds": 30,
    "EnableVerboseLogging": false,

    // Resilience Policies (Polly)
    "EnableRetryPolicy": true,
    "MaxRetryAttempts": 3,
    "RetryBaseDelaySeconds": 1.0,
    "EnableCircuitBreaker": true,
    "CircuitBreakerFailureThreshold": 5,
    "CircuitBreakerDurationSeconds": 30,

    // GraphQL-specific settings
    "GraphQLMaxTokens": 500,  // Prevents truncation. Use 200-300 for small models, 500-1000 for large models

    "SignalRPushIntervalMs": 5000,
    "HubContexts": [
      {
        "Name": "weather",
        "Description": "Realistic weather data with temperature, conditions, humidity, and wind speed for a single location",
        "IsActive": true
      },
      {
        "Name": "cars",
        "Description": "Vehicle status with plate number, location coordinates, speed, fuel level, and battery charge",
        "IsActive": false
      }
    ]
  }
}
